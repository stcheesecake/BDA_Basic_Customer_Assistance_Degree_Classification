# lightgbm_hpo.py
# -*- coding: utf-8 -*-

"""
Optuna(TPE) for current multiclass lightgbm_classifier.py

ìš”êµ¬ì‚¬í•­:
- ì‹¤í–‰ ì‹œ ì½˜ì†”ì—ëŠ” tqdm ì§„í–‰ë°”ë§Œ í‘œì‹œ (ì¶”ê°€ ìš”ì•½ print ì „ë¶€ ì œê±°)
- lightgbm_classifier.pyëŠ” ì–´ë–¤ ì‚°ì¶œë¬¼ë„ ìƒì„±í•˜ì§€ ì•ŠìŒ (produce_artifacts=False)
- ê²°ê³¼ëŠ” ë‹¨ í•˜ë‚˜ì˜ CSVë§Œ ìƒì„±: results/optimization_lightgbm/YYMMDD_hhmmss_hpo.csv
- ëª©ì í•¨ìˆ˜: f1_macro ìµœëŒ€í™”
"""

TRIALS = 100

import os
import csv
import argparse
from datetime import datetime
import contextlib
import io
import json
import numpy as np
import optuna
from optuna.samplers import TPESampler
from tqdm import tqdm
optuna.logging.set_verbosity(optuna.logging.WARNING)

# [ìˆ˜ì •] íŠœë‹í•  ëª¨ë¸ ìŠ¤í¬ë¦½íŠ¸ import
import lightgbm_classifier

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²€ìƒ‰ ë²”ìœ„ (LightGBM ìš©ìœ¼ë¡œ ìˆ˜ì •) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SEARCH_SPACE = dict(
    n_estimators=("int", 1500, 1700, 100),
    learning_rate=("float", 0.02, 0.05, None),  # Noneì€ log-uniform íƒìƒ‰
    num_leaves=("int", 145, 155, 1),
    max_depth=("int", 17, 18, 1),
    min_child_samples=("int", 99, 101, 1),
    subsample=("float", 0.8, 1.0, 0.2),
    colsample_bytree=("float", 0.9, 1.0, 0.1),
    reg_alpha=("float", 0.5, 1.5, None),  # L1 ì •ê·œí™” (log)
    reg_lambda=("float", 1e-3, 0.5, None),  # L2 ì •ê·œí™” (log)
)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Optuna Objective
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _suggest_params(trial: optuna.Trial) -> dict:
    """SEARCH_SPACEì— ì •ì˜ëœ ë²”ìœ„ì— ë”°ë¼ trial íŒŒë¼ë¯¸í„°ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤."""
    params = {}
    for name, (dtype, low, high, step) in SEARCH_SPACE.items():
        if dtype == "int":
            params[name] = trial.suggest_int(name, low, high, step=step)
        elif dtype == "float":
            if step is None:  # ë¡œê·¸ ìŠ¤ì¼€ì¼
                params[name] = trial.suggest_float(name, low, high, log=True)
            else:
                params[name] = trial.suggest_float(name, low, high, step=step)
    return params


def objective(trial: optuna.Trial, args, csv_path):
    """
    Optunaì˜ ê° trialì—ì„œ í˜¸ì¶œë˜ëŠ” ëª©ì  í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ì§€ì •ëœ íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  f1_macro ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # íŒŒë¼ë¯¸í„° ì œì•ˆ
        params = _suggest_params(trial)

        # HPO ì¤‘ì—ëŠ” ìƒì„¸ ë¡œê·¸ë¥¼ ì¶œë ¥í•˜ì§€ ì•Šë„ë¡ verbose=-1 ì¶”ê°€
        params["verbose"] = -1

        # [ìˆ˜ì •] lightgbm_classifier.train_and_eval í˜¸ì¶œ
        # HPO ì¤‘ì—ëŠ” í‘œì¤€ ì¶œë ¥ì„ ëª¨ë‘ ë¬´ì‹œí•˜ì—¬ tqdm ì§„í–‰ë°”ë§Œ ë³´ì´ê²Œ í•¨
        buf = io.StringIO()
        with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):
            out = lightgbm_classifier.train_and_eval(
                train_path=args.train_path,
                target_col="support_needs",
                save_dir=".",  # ì˜ë¯¸ ì—†ìŒ (íŒŒì¼ ì €ì¥ ì•ˆ í•¨)
                valid_size=args.valid_size,
                seed=args.seed,
                use_gpu=args.use_gpu,
                params_dict=params,  # dict ì§ì ‘ ì „ë‹¬
                produce_artifacts=False  # ğŸ”´ ì–´ë–¤ íŒŒì¼ë„ ìƒì„±í•˜ì§€ ì•ŠìŒ
            )

        metrics = out["metrics"]
        f1_macro = float(metrics.get("f1_macro", float("nan")))
        accuracy = float(metrics.get("accuracy", float("nan")))

        # ì¦‰ì‹œ CSVì— ê¸°ë¡
        row = [trial.number] + [params[k] for k in SEARCH_SPACE.keys()] + [f1_macro, accuracy]
        with open(csv_path, "a", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f)
            writer.writerow(row)

        return f1_macro

    except optuna.TrialPruned:
        raise
    except Exception as e:
        print(f"Trial #{trial.number} failed: {e}")
        return float("nan")  # ì‹¤íŒ¨í•œ trialì€ NaN ë°˜í™˜


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--train_path", default="data/train.csv")
    ap.add_argument("--n_trials", type=int, default=TRIALS)
    ap.add_argument("--valid_size", type=float, default=0.2)
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--use_gpu", action="store_true")
    args = ap.parse_args()

    # ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
    timestamp = datetime.now().strftime('%y%m%d_%H%M%S')
    save_dir = "results/lightgbm_optimization"  # [ìˆ˜ì •]
    os.makedirs(save_dir, exist_ok=True)
    csv_path = os.path.join(save_dir, f"{timestamp}_hpo.csv")

    # CSV í—¤ë” ì‘ì„±
    header = ["trial"] + list(SEARCH_SPACE.keys()) + ["f1_macro", "accuracy"]
    with open(csv_path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(header)

    # Optuna Study ìƒì„± ë° ìµœì í™” ì‹¤í–‰
    sampler = TPESampler(seed=args.seed)
    study = optuna.create_study(direction="maximize", sampler=sampler)

    # tqdmì„ ì‚¬ìš©í•œ ì§„í–‰ë°” í‘œì‹œ
    with tqdm(total=args.n_trials, desc="Optimizing") as pbar:
        def callback(study, trial):
            pbar.update(1)

        study.optimize(
            lambda trial: objective(trial, args, csv_path),
            n_trials=args.n_trials,
            callbacks=[callback]
        )

    # ìµœì¢… ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\n\n===== HPO ì™„ë£Œ =====")
    print(f"ì´ Trial: {len(study.trials)}")
    print(f"ìµœê³  ì ìˆ˜ (f1_macro): {study.best_value:.4f}")
    print("ìµœì  íŒŒë¼ë¯¸í„°:")
    for key, value in study.best_params.items():
        print(f"  - {key}: {value}")

    # ìµœê³  íŒŒë¼ë¯¸í„°ë¥¼ json íŒŒì¼ë¡œ ì €ì¥
    best_params_path = os.path.join(save_dir, f"{timestamp}_best_params.json")
    with open(best_params_path, 'w') as f:
        json.dump(study.best_params, f, indent=4)
    print(f"\nìµœì  íŒŒë¼ë¯¸í„°ê°€ '{best_params_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ì „ì²´ ê²°ê³¼ëŠ” '{csv_path}'ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()