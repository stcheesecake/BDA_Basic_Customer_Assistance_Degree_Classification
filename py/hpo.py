# hpo.py
# -*- coding: utf-8 -*-

"""
Bayesian Optimization (TPE) for CatBoost via model.train_and_eval()

- íƒìƒ‰ ê¸°ì¤€: balacc ìµœëŒ€í™”
- ê¸°ë¡ ê¸°ì¤€: F1 ìµœê³ (best_trial.txt), balacc ìµœê³ (best_params.json)
- í™”ë©´ ì¶œë ¥: tqdm ì§„í–‰ë¥ ë°” 1ì¤„ + postfix 1ì¤„(ìë™ ì¤„ë°”ê¿ˆì€ ì½˜ì†”ì— ë§¡ê¹€)
- model.pyëŠ” produce_artifacts=False, quiet=True ë¡œ í˜¸ì¶œ
- ê° trial ê²°ê³¼ë¥¼ ì¦‰ì‹œ CSVì— append (ì¤‘ë„ ì¢…ë£Œ ëŒ€ë¹„)
"""

import os
import io
import json
import csv
import argparse
from datetime import datetime
import contextlib

import pandas as pd
from tqdm import tqdm
import optuna
from optuna.samplers import TPESampler


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (ğŸ”§) ìµœìƒë‹¨ì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ì™€ trial ìˆ˜ë¥¼ ì§€ì •
# ê° í•­ëª©ì€ [start, end, step] í˜•ì‹
PARAMS_CONFIG = {
    "iterations":          [1000, 3000, 200],  # int
    "learning_rate":       [0.1, 0.3, 0.01],   # float
    "depth":               [6, 9, 1],          # int
    "l2_leaf_reg":         [1.0, 20.0, 0.5],   # float
    "border_count":        [50, 200, 10],      # int
    "random_strength":     [1.0, 5.0, 0.1],    # float
    "bagging_temperature": [0.1, 0.9, 0.05],   # float

    # â¬‡ï¸ ì¶”ê°€: SMOTE-NC í•˜ì´í¼íŒŒë¼ë¯¸í„°ë„ íƒìƒ‰
    "smote_sampling":      [0.80, 1.00, 0.05], # float (ì†Œìˆ˜:ë‹¤ìˆ˜ ëª©í‘œ ë¹„ìœ¨)
    "smote_k":             [3, 9, 1],          # int   (k_neighbors)
}
TRIALS = 2000
SEP = " | "  # í•œ ì¤„ ì¶œë ¥ìš© êµ¬ë¶„ì
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--train_path", default="data/preprocessed_train_oof.csv")
    ap.add_argument("--save_dir", default="results/optimization")
    ap.add_argument("--csv_path", default="results/optimization/bo_trials.csv")
    ap.add_argument("--params_json_out", default="results/optimization/best_params.json")
    ap.add_argument("--best_txt_out", default="results/optimization/best_trial.txt")
    ap.add_argument("--valid_size", type=float, default=0.2)
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--deterministic", action="store_true")
    ap.add_argument("--trials", type=int, default=TRIALS)
    return ap.parse_args()


def _ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)


def _suggest_from_config(trial: optuna.Trial, name: str, cfg):
    s, e, st = cfg
    if float(st).is_integer() and float(s).is_integer() and float(e).is_integer():
        return trial.suggest_int(name, int(s), int(e), step=int(st))
    return trial.suggest_float(name, float(s), float(e), step=float(st))


def build_params_from_trial(trial: optuna.Trial) -> dict:
    params = {k: _suggest_from_config(trial, k, cfg) for k, cfg in PARAMS_CONFIG.items()}
    params.update({
        "task_type": "GPU",
        "boosting_type": "Ordered",
        "bootstrap_type": "Bayesian",
        "verbose": False,
    })
    return params


def main():
    # Optuna INFO ë¡œê·¸ ìˆ¨ê¹€
    optuna.logging.set_verbosity(optuna.logging.WARNING)

    args = parse_args()
    _ensure_dir(args.save_dir)

    # ê°™ì€ í´ë”ì˜ model.py ì‚¬ìš©
    import model

    # â”€â”€ (NEW) ì‹¤í–‰ë§ˆë‹¤ ì „ìš© í´ë”/íŒŒì¼ ê²½ë¡œ ë§Œë“¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    run_tag = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = os.path.join(args.save_dir, run_tag)
    _ensure_dir(results_dir)

    # ì…ë ¥ ì¸ìì— ìˆë˜ ê²½ë¡œë“¤ì€ ê·¸ëŒ€ë¡œ ë‘ë˜, ì‹¤ì œ ì“°ê¸°ëŠ” run ì „ìš© íŒŒì¼ë¡œ
    run_csv_path       = os.path.join(results_dir, f"{run_tag}_bo_trials.csv")
    run_params_json    = os.path.join(results_dir, f"{run_tag}_best_params.json")
    run_best_txt       = os.path.join(results_dir, f"{run_tag}_best_trial.txt")

    # CSV í—¤ë” ìƒì„± (ì‹¤í–‰ë§ˆë‹¤ ìƒˆë¡œìš´ íŒŒì¼)
    columns = [
        "trial", *[f"param_{k}" for k in PARAMS_CONFIG.keys()],
        "best_threshold", "policy",
        "f1", "auc", "precision", "recall",
        "acc0", "acc1", "balacc", "youden",
        "score", "seed",
    ]
    with open(run_csv_path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f, delimiter=",", quoting=csv.QUOTE_NONNUMERIC, lineterminator="\n")
        writer.writerow(columns)

    # ë² ìŠ¤íŠ¸ íŠ¸ë˜ì»¤
    best_balacc = -1.0
    best_by_balacc = None  # (trial_no, params, metrics, thr)
    best_f1 = -1.0
    best_by_f1 = None      # (trial_no, params, metrics, thr)

    # â”€â”€ ì§„í–‰ë¥ ë°” 1ì¤„ë§Œ ì‚¬ìš© (ë§‰ëŒ€ ê¸¸ì´ ê³ ì • 4ì¹¸; ë‚˜ë¨¸ì§€ëŠ” ì½˜ì†”ì´ ìë™ ì¤„ë°”ê¿ˆ)
    pbar = tqdm(
        total=args.trials,
        desc="Bayesian Optimization (TPE)",
        dynamic_ncols=True,
        bar_format=(
            "{desc:<26} "
            "{percentage:3.0f}%|{bar:4}| "
            "{n_fmt}/{total_fmt} [{elapsed}<{remaining}]  {postfix}"
        ),
        position=0,
        leave=True,
    )

    def objective(trial: optuna.Trial):
        nonlocal best_balacc, best_by_balacc, best_f1, best_by_f1

        params = build_params_from_trial(trial)

        # (NEW) SMOTE-NC ë©”íƒ€í‚¤ ë§¤í•‘: model.train_and_evalì´ ì¸ì‹
        #  - PARAMS_CONFIGì˜ smote_* ê°’ì„ popí•´ CatBoostë¡œ ì „ë‹¬ë˜ì§€ ì•Šê²Œ í•¨
        params["_use_smote_nc"]   = True
        params["_smote_sampling"] = float(params.pop("smote_sampling"))
        params["_smote_k"]        = int(params.pop("smote_k"))

        # model.py ì¶œë ¥/ì €ì¥ ì°¨ë‹¨
        buf = io.StringIO()
        with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):
            out = model.train_and_eval(
                train_path=args.train_path,
                params=params,
                target_col="withdrawal",
                save_dir=results_dir,          # (NEW) ì´ ì‹¤í–‰ ì „ìš© í´ë”
                valid_size=args.valid_size,
                seed=args.seed,
                deterministic=args.deterministic,
                produce_artifacts=False,       # íŒŒì¼ ìƒì„± X
                quiet=True,                    # ì½˜ì†” ì¶œë ¥ X
            )

        thr = float(out["threshold"])
        m = out["metrics"]
        f1 = float(m.get("f1", float("nan")))
        balacc = float(m.get("balacc", float("nan")))
        policy = getattr(model, "THRESHOLD_STRATEGY", "balacc")

        # CSV ì¦‰ì‹œ append ì €ì¥ (quoted)
        param_values = {f"param_{k}": trial.params.get(k, None) for k in PARAMS_CONFIG.keys()}

        row = {
            "trial": trial.number,
            **param_values,
            "best_threshold": thr, "policy": policy,
            "f1": m.get("f1"), "auc": m.get("auc"),
            "precision": m.get("precision"), "recall": m.get("recall"),
            "acc0": m.get("acc0"), "acc1": m.get("acc1"),
            "balacc": m.get("balacc"), "youden": m.get("youden"),
            "score": m.get("score"),
            "seed": args.seed,
        }
        with open(run_csv_path, "a", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f, delimiter=",", quoting=csv.QUOTE_NONNUMERIC, lineterminator="\n")
            writer.writerow([row[c] for c in columns])

        # ë² ìŠ¤íŠ¸ ê°±ì‹ 
        if balacc > best_balacc:
            best_balacc = balacc
            best_by_balacc = (trial.number, {**trial.params}, m, thr)
        if f1 > best_f1:
            best_f1 = f1
            best_by_f1 = (trial.number, {**trial.params}, m, thr)

        # â”€â”€ ì¶œë ¥: ì§„í–‰ë¥ ë°” í•œ ì¤„ì˜ postfix ë¡œë§Œ ê°±ì‹ 
        postfix = SEP.join([
            f"thr:{thr:.4f}",
            f"f1:{float(m.get('f1', float('nan'))):.4f}",
            f"bal:{float(m.get('balacc', float('nan'))):.4f}",
            f"auc:{float(m.get('auc', float('nan'))):.4f}",
            f"pre:{float(m.get('precision', float('nan'))):.4f}",
            f"rec:{float(m.get('recall', float('nan'))):.4f}",
            f"a0:{float(m.get('acc0', float('nan'))):.4f}",
            f"a1:{float(m.get('acc1', float('nan'))):.4f}",
            f"you:{float(m.get('youden', float('nan'))):.4f}",
            f"sc:{float(m.get('score', float('nan'))):.4f}",
            f"sd:{args.seed}",
        ])
        pbar.set_postfix_str(postfix, refresh=True)
        pbar.update(1)

        # ìµœì í™” ê¸°ì¤€: balacc
        return balacc

    # Optuna study
    study = optuna.create_study(direction="maximize", sampler=TPESampler(seed=args.seed))

    try:
        study.optimize(objective, n_trials=args.trials, show_progress_bar=False, gc_after_trial=True)
    except KeyboardInterrupt:
        print("\n[Warn] Interrupted by user. Writing artifacts collected so far...")
    finally:
        pbar.close()

        # ì €ì¥ë¬¼
        if best_by_balacc is not None:
            tno, params_used, metrics, thr = best_by_balacc
            payload = {
                "params": {
                    **params_used,
                    "_use_smote_nc": True,
                    "_smote_sampling": float(params_used.get("smote_sampling")),
                    "_smote_k": int(params_used.get("smote_k")),
                },
                "best": {
                    "trial": int(tno),
                    "threshold": float(thr),
                    "policy": getattr(model, "THRESHOLD_STRATEGY", "balacc"),
                    "metrics": metrics,
                },
                "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "selection_metric": "balacc",
            }
            with open(run_params_json, "w", encoding="utf-8") as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
            print(f"\n[Saved] best params (by balacc) -> {run_params_json}")

        if best_by_f1 is not None:
            tno, params_used, metrics, thr = best_by_f1
            lines = [
                "==== Best Trial (by F1) ====",
                f"trial: {tno}",
                f"Best threshold : {thr:.4f} (policy: {getattr(model, 'THRESHOLD_STRATEGY', 'balacc')})",
                "",
                "[Metrics]",
            ]
            for k in ["f1", "auc", "precision", "recall", "acc0", "acc1", "balacc", "youden", "score"]:
                v = metrics.get(k, None)
                if v is not None:
                    try:
                        lines.append(f"{k}: {float(v):.6f}")
                    except Exception:
                        lines.append(f"{k}: {v}")
            lines.append("")
            lines.append("[Params]")
            # trial.paramsì—ëŠ” smote_*ê°€ ë“¤ì–´ìˆìŒ
            for k in sorted(params_used.keys()):
                lines.append(f"{k}: {params_used[k]}")
            with open(run_best_txt, "w", encoding="utf-8") as f:
                f.write("\n".join(lines))
            print(f"[Saved] best trial (by F1) -> {run_best_txt}")


if __name__ == "__main__":
    main()
